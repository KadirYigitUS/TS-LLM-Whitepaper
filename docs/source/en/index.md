---
title: Outline — Extended Foundational Models of LLMs for Translator Studies
---

# Outline — Extended Foundational Models of LLMs for Translator Studies

> Intended audience: Translation Studies (TS) scholars who are fluent in qualitative theory yet new to NLP/LLM engineering.

## 0. Frontmatter and Usage Notes
- **Skopos**: Bridge the engineering history of LLMs with translator-centered theory.
- **Vault linkage**: Mirrors the Obsidian structure so each section stays modular for future remixing.

## 1. Introduction
### 1.1 Motivation
- Translators confront generative AI every day but still describe it as a black box.
- Demystification replaces "AI as Magic" with "AI as probability, vectors, and statistics".

### 1.2 Epistemic Gap Analysis
- **Gap**: Minimal exposure to statistical linguistics, embeddings, or neural topologies.
- **Bridge**: Reframe those concepts using TS analogies (e.g., "vector spaces" become "semantic equivalence fields").

## 2. Theoretical Foundations (Secondary Sources)
The vocabulary required before the nine seminal papers.

### 2.1 From Rules to Statistics
- **Statistical Linguistics**: Markov chains and n-grams predict the next word.
- **TS Parallel**: Descriptive Translation Studies already values corpora over prescriptive rules.

### 2.2 Neural Foundations
- **Perceptrons & Neural Nets**: Biological metaphors vs. actual matrix multiplication.
- **Backpropagation**: Error minimization built into every translation pass.
- **Embeddings**: Meaning is a vector’s position, not a dictionary entry.

## 3. Evolutionary Arc of LLMs — The 9 Seminal Papers
Each stage is mapped to a TS-friendly analogy.

1. **Attention Is All You Need** (2017) — Birth of the Transformer via self-attention.
2. **BERT** (2018) — Bidirectional context with masked language modeling.
3. **GPT-3** (2020) — Emergent capabilities through sheer scale.
4. **Scaling Laws** (2020) — Power laws make progress predictable.
5. **LLaMA** (2023) — Data-optimal, open models for universities.
6. **Chain-of-Thought** (2022) — Prompting as cognitive scaffolding.
7. **InstructGPT** (2022) — RLHF aligns probability with intent (Skopos alignment).
8. **Flan Collection** (2022) — Massive instruction tuning for generalization.
9. **Toolformer** (2023) — LLMs as API-using agents.

## 4. Critical Intersections: AI & Translation Theory
- **Prompt Engineering as Skopos**: System instructions are translation briefs. See `getting_started.md`.
- **Context Engineering**: Halliday’s Field/Tenor/Mode become explicit prompt slots.
- **Empiricism & Critique**: Shift from error counting to prompt/process auditing.

## 5. Discussion
Translators evolve from dictionary users to probabilistic auditors who can interrogate the model’s choices.

## 6. Future Directions
- **New Metrics**: Prompt sensitivity, hallucination rate, cultural erasure.
- **Study Designs**: Longitudinal translator+AI ethnographies.

## 7. Appendices & References
- **Glossary**: `general_terminology.md`
- **Bibliography**: `references.md`
- **Visual assets**: `knowledge_graphs.md`

```{seealso}
Additional linked notes:
- [Beyond Prompt Engineering](obsidian_workflow.md) — operationalizing Halliday.
- [Combined Source Review](build_pipeline.md) — detailed reading notes for every seminal paper.
- [Discussions](discussions.md) — synthesis for seminars and panels.
- [Future Studies](future_studies.md) — grant-ready experimental ideas.
```
